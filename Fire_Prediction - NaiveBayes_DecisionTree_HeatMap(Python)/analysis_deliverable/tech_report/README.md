# Tech Report
Decision Tree prediction task with visualized table and tree. Note: Included Naive Bayes model and results in our repo but this is not the focus of our disucssion.

Note: we included a visualization of naive bayes results and a heatmap of where fires occur as well (as these are other things we have explored / are exploring), but focus of report is on decision tree prediction task and visualization of these results.

### ML Prediction Task ###
In our data, roughly 90% of fires were of 1 of 3 types. Structural, mobile vehicle, or natural vegetation.  The other types of fires, such as "mobile property used as a structure" and "cultivated vegetation" we thought would likely occur in similar conditions to one of the three main fires mentioned (i.e. mobile property and structure fires are started in similar ways and conditions). Moreover, these fires are treated using similar tools, thus if you believe structure fires might occur on a given day due to the weather, and a fire house is preparing for that, they are also preparing for mobile property fires. So, we chose to use weather data to try and predict what kind of fire would occur in an instance where there was a fire. Thus, an application of this model, is that on a given day, a fire house could plug in the weather for the day into our model, and they could see what kind of fire is most likely to occur that day. This would allow them to better prepare to fight fires that day.

Goal: Given a structure, vehicle, or natural vegetation fire, use weather data to predict what kind of fire occurred.

Metric for Success: The precision, recall, and F1 score on the overall test data give us insight into how out model is performing, but test accuracy is the metric we care most about as we simply want to correctly identify the most cases of which fire occurred. We felt noticeably beating 33% in accuracy would be some sort of victory as this would be higher than 1/3 (picking a point by chance), 50% would be a solid model, and 75% would be a great model.

### Model Choice, Measures, Challenges, Cleaning ###

#### Model Choice ####
We used a decision tree ML algorithm firstly because it made no assumptions on the data. We are unsure of the relationships between different attributes and between different attributes and the labels, so we wanted to make sure that at least for one model we made no assumptions about our data. Moreover, we thought the visualized output of the decision tree would be easily digestible.  We could see what weather factors typically bear the most importance in deciding which fire might occur on a given day. We also considered (and did use) Naïve Bayes. Although this algorithm makes some assumptions on independence of attributes ( which we think do likely not entirely hold here) we felt that it could be useful in its predictive power, and we knew in practice sometimes it is used despite the independence assumption being slightly violated. Again, we felt relying on decision tree as our main algorithm given its insightful visualizations, lack of assumptions was a good idea, and the eventually higher F1 and accuracy scores validated this decision further. 

#### Measures success ####
We measured success or failure by accuracy as mentioned. We also would have liked to see similar scores across all three fires so that our model did not routinely make more mistakes on certain types of fires (this turns out to be a possible flaw in our model). We used the metric of accuracy score because the goal of our tasks is to simply predict the most fires correctly, so that if a fire house is using our model, we are maximizing the days where they are preparing to fight the correct kind of fire.  The challenge we faced when measuring model success was that we weren’t sure how our model might be performing relative to other models. We had decent accuracy, but we were not sure if that meant this was a good model choice on data that didn’t have much predictive relations in it, or if we had chosen a bad model and other models would far outperform the decision tree in a prediction task. This is to say we were unsure of how to gauge the validity of our choice of ML algorithm, as we had no other predictors to compare it to. This is why we implemented Naïve Bayes, and we saw the decision tree perform slightly better based on the accuracy discussed.  As a quick note on our treen, we tweaked the depth of the decision tree until we achieved the highest accuracy at depth 6, and this also corresponded to training and testing accuracy being the closest (avoiding overfitting that can occur at higher depths). 

#### Cleaning ####
We had to clean our data quite a bit. Sticking to what we had to do for the decision tree, we first had to group labels of fires into one of the three structure, vehicle, or natural vegetation category. The labels were all numerical, and numbers in a given range, i.e. 111-118, represented specific types of structure fires. So, we had to convert all those numerical labels into the appropriate categorical label. We also had to one-hot our attribute data. In order to make a decision tree we had to turn continuous data, in a column like temperature, into discrete data, i.e temperature between 50-70. We also had to remove two weather attributes that seemed redundant (low temperature and high temperature since we already had temperature), and an attribute that was very sparsely populated (heat index). Finally, we dropped any row (representing a fire) that was missing any of the included attributes. We checked and this did not affect the overall distribution of fires in our data. Although, we still had over 50,000 data points after this so our sample size remained strong, it is worth mentioning these missing attributes did sometimes correspond to a certain zip-code which might somehow influence our predictions. 


### Interpretation ###
Our test accuracy was .55. This means that based on weather attributes, in 55% of cases our model correctly identified whether a fire was a structure, vehicle, or natural vegetation fire.  The model performed best on natural vegetation and structure fires, but poorly on vehicle fires as shown by a low recall score. We think this might have happened in part because Vehicle fires made up the smallest portion of the training/testing data (20%), so the leaves in our decision tree were just less likely to label a vehicle fire.  This resulted in solid precision (when there was a leaf saying vehicle fire it was correct 54% of the time) but a recall of basically 0 (there were not nearly enough leaves identifying a fire as a vehicle fire). We also had the best recall on structure fires which made up about 50% of the training/testing data. So, using the same logic as vehicle fires but in the opposite scenario, we had many leaves identifying a fire as a structure fire given its frequency in the data, but many of these classifications were wrong (structure had a higher precision than recall). This is a solid result overall.  We were able to predict much better than chance which of the three kinds of fires would occur. Our decision tree also split on some intuitive attributes, as shown in the visualization. For example, brush fires are more likely to occur when there is low humidity. We can see at depth 3 on the decision tree, that our model splits on humidity, and natural vegetation is the identified fire type, for the lower end of humidity, at that split. In general, I am moderately confident in the results. Our model achieved an accuracy that is much better than chance, and better than identifying all fires as one type i.e. structure, so at the very least it is useful. However, I think it could be improved. If we trained on equal amounts of all kinds of fires, I am curious if we could achieve a better prediction accuracy, and this is something that we might do for the poster.  This might help us to eliminate the low recall for vehicles, and the overly high recall for structure fires that may have occurred due to overrepresentation of structure data in our dataset.


### Visualization ###
#### Tables ####
For the visualization we chose to show tables representing our models’ train and test precision, recall, F1 and accuracy across all labels and we printed two decision tree visualizations, one of which we will use in the poster.  We wanted to show both tables to show that our model was not necessarily over or under fitting. We also included the results of the Naïve Bayes so we could compare results to another algorithm. Within the tables, we felt showing precision, recall, F1, and accuracy was important as precision and recall gave us insight into how our model might be flawed (as discussed above) and F1 is a commonly used metric that punishes both false positives and negatives.  We absolutely wanted accuracy included as this was the metric we cared most about. Finally, we thought it was helpful to have a weighted average and macro average, as this helped illuminate that our model’s success might be slightly driven by the overrepresentation of structure fires in our data, as shown by higher weighted scores versus average scores.

Here is the confusion matrix that resulted from training our decision tree with max_depth set to 6:
![alt text](https://github.com/CS1951A-S21-Brown/fire_guys_final_project/blob/main/analysis_deliverable/visualizations/decision_tree_train_depth_6.png) 

Here is the confusion matrix that resulted from testing (on our decision tree with max_depth set to 6): 
![alt text](https://github.com/CS1951A-S21-Brown/fire_guys_final_project/blob/main/analysis_deliverable/visualizations/decision_tree_test_depth_6.png)

Additionally, here are the links to the [training confusion matrix](https://github.com/CS1951A-S21-Brown/fire_guys_final_project/blob/main/analysis_deliverable/visualizations/naive_bayes_train.png) and the [testing confusion matrix](https://github.com/CS1951A-S21-Brown/fire_guys_final_project/blob/main/analysis_deliverable/visualizations/naive_bayes_test.png) for our Naive Bayes model.

#### Trees ####
 We chose to include the depth 5 decision tree visualization to include depths at which we had at least one leaf that corresponded to each label. We could not format it so that the bottom nodes were readable in the frame, but you are able to see the labels at every node, and we wanted to simply show a tree where vehicle classifications were included. This will not be used in the poster, but was just to show our algorithm was running effectively. So we chose to also include the depth 3 tree as this is what we will likely use in our actual poster. At depth 3, we can actually fit all leaf nodes in the picture frame without being cut off, and it is small enough where we can read what is on each node. To describe what we included on each node, at the top we have the attribute we split on, then we have the total training data points at the node, we have the number of data points for each fire label remaining at each node in alphabetical order [“Natural Vegetation”, “Structure”, “Vehicle”], and we have the most common label listed at the bottom of each node.  The nodes are color coded by how frequent the most common label is at each node, green for structure, red for natural vegetation, and purple for vehicle. The higher proportion that the majority label represents at each node, the bold the color of that node. The bottom nodes are leaf nodes, and thus do not have an attribute to split on but instead just have a classification.  I think we could have communicated the results from the table differently using bar charts to show the relative values of different metrics between labels. This might help to better illuminate how our model performed across different fire labels. We also could have made a heat-map excel spreadsheet that highlighted the best values in green and the worst in red to draw the eye to what metrics our model performed well on, and which ones it did not perform so well on. There were no challenges in visualizing the table, but we did have challenges with what to show at each node on the decision tree. We settled on the information that we thought gave insight into how our model made predictions based on attributes (attribute split), and how labels were separated by each attribute (data at each node, data by label at each node, and classification label). We also went for clarity, and readability in case the text was small! Hence the color coding. Also, I mentioned we could not fit the larger depth tree within frame without overlap, but again we wanted to include it to show our model was in fact producing some vehicle leaves.  I think our table may warrant a brief description of what each metric represents and perhaps what the values of each metric show about our model. Our decision tree I believe also might need a color-coded key, and a quick description of what the information listed at each node represented (especially the list of values listed 3rd as that was not originally clear to us).

Below are our two visualizations (the first with max_depth set to 3, the second with max_depth set to 5):

![alt text](https://github.com/CS1951A-S21-Brown/fire_guys_final_project/blob/main/analysis_deliverable/visualizations/depth_3_tree.png)
![alt text](https://github.com/CS1951A-S21-Brown/fire_guys_final_project/blob/main/analysis_deliverable/visualizations/depth_5_tree.png)
 
### Selection/Sensitive Attributes ###

#### Selection ####

As mentioned above, we chose this model because it made no assumptions on the data, and we felt we did not have enough information to make assumptions on the data. Moreover, the results of a decision tree, especially when visualized, can provide interesting insights into what specific weather attributes are most important in  determining whether a structure, vehicle, or natural vegetation fire will occur.  Finally, it does seem that the model makes intuitive sense in being used to  classify which fire might occur. We thought there might be cutoffs between certain temperatures, wind speeds, or humidities that made certain types of fires more or less likely to occur.

#### Sensitive Attributes ####
For the ML model I do not think we had any direct sensitive attributes that would affect our output. Weather data is, by definition, natural so there were likely no instances of societal bias present directly in our training attributes. However, in general, our overall data does reflect sensitive attributes. For example, structure fires are more common in low income  urban communities as there is worse infrastructure, and closer living conditions. This was discussed in depth in our impact report.  But, just to conclude, it does not seem immediately clear that weather causing types of fires had much influence from sensitive attributes.


### Viz Discussion/Future Plans
The visualizations were discussed in depth above. In brief, low recall on vehicle, and high recall with lower precision on structures from the test table visualization might have showed that our model was being influenced by an overrepresentation of structure fires. The visualization of our tree showed which attributes best separated the data by fire label. Relative humidity, temperature, cloud cover, and snow depth were the first attributes split on implying that these attributes best separated the data. Relative humidity was an example of an intuitive split as in low humidity conditions brush fires (natural vegetation) are more likely to occur.  In the poster we will likely include just the test table and mention how training was slightly better.  The test table is what is important for evaluating our results. Moreover, we will include only the smaller depth decision tree with a brief description of what is listed on the nodes, and a color coding.  For the future direction of the project we will likely have a poster with three columns: “where” “when” and “why” fires occur. In the where column we will have a heat map of fires per capita to show where all types of fires are most common across the United States. This will give us a sense of where fires are a real issue. In our when column we will do an ANOVA test to see which fire types, if any, vary across seasons. We will have statistical tables showing results and perhaps bar charts visualizing when certain fire types occur across the 4 seasons. Finally, in our why column, we will include the decision tree test table and tree, and a naïve bayes test table to show how two different ML models perform in trying to predict the three major types of fires.