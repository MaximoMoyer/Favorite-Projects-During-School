# Code README
Below are in depth design decisiong and documentation on how to run any of the above code


#### Decision Tree ####

##### Design Decisions #####
In Decision Tree we first read in the file, then removed two columns from the data that had repetitive attributes (high temperature, and low temperature, since we already had temperature), and heat index as there was very sparse data for this attribute. Then we removed all the rows (each row represents one instance of a fire) with no label or a label of 100 which signified "other" type of fire.  Then, we removed all rows that did not have a label corresponding to one of the three main types of fires (natural vegetation, structure, and vehicle).  We then dropped all rows that did not have complete data. We ended up with over 50,000 data points, each with 11 attributes after all this data cleaning. Moreover, we found it did not affect the general distribution of vehicle vs. nat. veg. vs. structure fires in our data. Thus, we felt comfortable with the data cleaning we conducted. In order to run our decision tree, we then had to one hot our weather type and condition type columns, since they were categorical variables.  Although this meant work, we felt it was important to include these variables in the analysis as they might have influenced what type of fire would occur. We decided to create a separate function for this as it was a lot of code. This function took in all attribute data, and returned data that broke out the weather type and conditions columns into multiple columns, where each new column was a possible input to each of these respective columns and had a 1 in it if the respective fire used to have the new column’s title listed as the categorical variable under weather type or conditions, and a 0 otherwise. Thus, each row now had a 1 in just one of the newly added weather type and a 1 in just one of the newly added conditions columns. After this was down, we separated our data in 80/20 train test split. We did this using a random mechanism to ensure there was no bias in our training data. And then, we trained and fit our decision tree classifier, then printed out the results of both training and testing data. Finally, after writing all this code we tweaked our decision tree depth (we started at 11) until our training data and testing data were as close as possible, for the highest possible value of testing data. This is to say that as we lowered our tree depth from 11, our testing measures improved, and our training measures got worse. This signifies we were likely overfitting at larger depths. Then, once we decreased past 6, our testing measures also began to worsen, thus we felt 6 was our optimal depth for testing performance. For our visualization, we changed the depth to 3, and the same attributes were still split on, we did this because at a depth of 6 it is very difficult to read the nodes on our tree!

As a quick mention, we felt a decision tree was appropriate for this prediction method as we could clearly visualize for the audience was attribute were most important in determining what fire might occur on a given day. Also, it made no assumptions about the underlying distributions of the data we were dealing with, which we felt was a positive given we do not have much of an understanding of how weather or fire data might be distributed. 



##### Run Instructions #####
If you want to run the decision tree on the three fire types mentioned, you simply need to download the Decistion Tree file and oneHot.py file alongside the comb_file.csv file (the file containing all our data).  Then, ensure that your file path matches up with where comb_file is stored and that oneHot’s method can be imported into your decision tree file, and then run!  If you want to run our decision tree for every type of fire (excluding “other” and none labels for fires) you simply have to comment out lines 39,40,41 and 43, and the uncomment lines 38, 42. If you want to change the fire types from the three we trained on or all fire types, you will have to manipulate pd.cut, and the labels. Moreover, if you want to change the attributes trained on you can simply remove them from the columns line on line 18, but make sure those attributes are not specifically mentioned in oneHot or you will get an error.  Note if you are running this file as is the decision tree will look nearly unintelligible at a depth of 6, and that the visualization takes times to show up.  You can lower the decision tree depth to get a nicer visualization.


#### Naive Bayes ####

##### Design Decisions #####
Naive Bayes carried out the exact same process of data cleaning as the decision tree, but did one extra step. We also had to "bucket" our data for continuous variables in order to run Bernoulli Naïve bayes. This meant that for each of the 9 continuous, quantitative weather attributes, we had to find the minimum, 33th percentile, 66th percentile, and the maximum. We then created new columns for each attribute representing that attribute from the minimum to the 33rd percentile, 33rd to the 66th percentile, and 66th percentile to the maximum. For each fire, we then filled in a single 1 into one of the 3 buckets for each attribute, depending on which bucket that row’s attribute fell into. So, we ended up for each attribute, deleting the original column name, creating three new columns each representing a “bucket” where two of the buckets would have a 0 and one of the buckets would have a 1 for each fire. We decided to make sure that there was an equal proportion of 1’s throughout each of the tree buckets for a given attribute, hence the use of equally spaced percentiles, because when we originally tried to create equally sized buckets, we found many attributes’ data was skewed one way such that nearly all the 1’s ended up in the same bucket.   Moreover, we felt 3 was an appropriate number of buckets for each attribute as it lent the intuitive meaning of “low” “medium” or “high” values for any given attributes. I.E High temperatures results in a higher probability of vehicle fire. We created a separate file for bucketing the data as it was a lot of code, and would make the readability of the code more confusing. After bucketing the data, we then did the same 80/20 train test split and ran an imported Naïve Bayes model, printing out both the training and testing results.   We felt a Naïve Bayes would be a good model to run as it can predict the likelihood of multiple classes, and you can learn a lot about which attributes have the most predictive power according to the model if you chose to dig into the output of the model given that it is a generative model.  One downside is that it makes an independence assumption between the attributes that might not necessarily exist. We still chose to run this model however as we learned that in practice often times Naïve Bayes can still be a useful prediction mechanism, even if the underlying assumption of independence among attributes is not wholly and completely satisfied.

As a quick mention, we decided not to save our cleaned data for either ML method, because 1) it ran very fast already so removing cleaning from the runtime had no large effect and 2) we wanted our model to be generalizable to be able to include different fire types, and different attributes, in case someone wanted to reuse our model in a different way. We also wanted someone to be able to run Naïve Bayes with different specifications than the decision tree, this is why we left preprocessing data in both files, in order to give a prospective user flexibility.

##### Run Instructions #####

If you want to run the Naïve Bayes model on the three fire types mentioned, you simply need to download the Naïve Bayes file, oneHot.py,Continuous_bucekts.py, and comb_file.csv file (the file containing all our data).  Then, ensure that your file path matches up with where comb_file is stored and that oneHot’s and Continuous Buckets method can be imported into your Naïve Bayes file, and then run!  If you want to run our Naïve Bayes for every type of fire (excluding “other” and none labels for fires) you simply have to comment out lines 39,40,41 and 43, and the uncomment lines 38, 42. If you want to change the fire types from the three we trained on or all fire types, you will have to manipulate pd.cut, and the labels. Moreover, if you want to change the attributes trained on you can simply remove them from the columns line on line 18, but make sure those attributes are not specifically mentioned in the oneHot or buckets methods or you will get an error.



#### Data Collection ####

Our data comes from FEMA's National Fire Incident Reporting System (NFIRS). This a voluntary reporting system for logging fire responses to 911 calls, and is used by about 79% of departments nationally. Annually, this comprises of several million calls, including fire, emergency medical service (EMS) calls, and motor vehicle collisions (MVCs). We requested this data from FEMA for the year 2018 last October, and found out it only is distributed by DVD. A combination of untraceable mail service, the global pandemic, and Brown's mail room resulted in a 3 month delay before we actually received the files. The data is extensively documented, and consists of 18 csv files containing various tables of different kinds of information. We decided to focus on the ~600,000 fire calls, and so only used three of these tables: basicincident, incidentaddress, and fireincident. basicincident consists of some high level information about each call, incidentaddress contains geographic information about where it occurred, and fireincident contains operational information of what happened. We joined these tables on the 5 primary keys they contained, and our resulting table had more than 150 columns.

To collect the weather data, we used the Visual Crossing APIto query each day and zip code for each fire call. This API granted us 10 million calls for $35, but it took us over 144 hours to collect the data for our 600,000 calls. The data gave us 10 weather parameters of each day, but was incomplete for about 1% our data that did not a weather station within 50 miles of that zip code, or didn't have a zip code. We made a decision to ignore this missing data, as it was a small fraction and wasn't likely to dramatically skew our results.

##### Design Decisions #####

We decide to store our baseline data in a single csv file, so after joining the 3 tables from the NFIRS dataset, we append the weather columns onto this table. While this new table was sizeable, it was also easy to randomly sample sevarl rows to experiment with before running something on the dataset as a whole. We made extensive use of the pandas library in python to accomplish our joins, and parsing/conversion of csv files.

##### Run Instructions #####

To recreate our joins, run nfirs.py. To query the weather data, run weather_scrape.py.


#### Heat Map ####

Our heatmap was created by simply counting the number of fires that occurred in each zip code, and dividing that count by the population of that zip code. This was done in ArcMap, after using a groupby to collect all the calls by zip code in Python. 

##### Design Decisions #####

For this map, we chose to do all the joins in ArcMap. This was because we knew we had incomplete data for the zip codes (a few were missing, some contained alphabetical characters) and ArcMap ignores these types gracefully, along with geographic support. We joined this table to a shapefile that mapped all the zip codes. We may have some small discrepancies due to having a more recent zip code map (2020), along with some minor disagreements between zip codes and ZCTA5s. We then join this shapefile to a csv of zip code populations in 2018, pop-by-zip-code.csv. We have at least one fire call in about 62% of zip codes. The ArcMap code then divides the count of fire calls by population, and displays each zip code by this calculated frequency.

##### Run Instructions #####

To recreate the heatmap, first run the python file fire_map.py. Next, open fire_map.mxd as an ArcMap file, and the map should populate, and can now be tweaked or displayed differently. This is a map exchange document that contains the US zip code shapefile, as well as the joined tables of zip code fires and zip code populations.


#### Seasonal Frequency Counts ####
##### Design Decisions #####
To get the seasonal frequency of each of the 7 fire incident types (structure, mobile property, vehicle, natural vegetation, rubbish, special outside, and cultivated vegetation), first, we cleaned the data to the point that all we had was the date of the incident and the type of incident. From there, we were able to loop through all of the incidents and check their date to figure out which season the incident fell into. After assigning each incident a season, we simply looped through the data again and retrieved the counts of each incident during each season. 

##### Run Instructions #####
To run the seasonal frequency code (this will simply print the counts in the terminal), the only requirement is to have the path of the 'comb_file.csv' file correct when initially reading in the data on line 30. Other than that, simply run the file in the terminal with the syntax 'python seasonal_frequency.py.'
